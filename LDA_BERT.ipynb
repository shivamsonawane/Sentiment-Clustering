{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivamsonawane/Sentiment-Clustering/blob/main/LDA_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " # !pip install stop_words\n",
        "# !pip install language_detector\n",
        "# !pip install symspellpy\n",
        "# !pip install umap\n",
        "# !pip install sentence_transformers\n",
        "# !pip install 'umap-learn==0.3.10'"
      ],
      "metadata": {
        "id": "1tSk1QJM6gjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NWsC3qR6Z3c",
        "outputId": "fe232169-0e9d-4364-b401-d4ffb598a66f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        }
      ],
      "source": [
        "from stop_words import get_stop_words\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('popular')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from language_detector import detect_language\n",
        "\n",
        "import pkg_resources\n",
        "from symspellpy import SymSpell, Verbosity\n",
        "\n",
        "sym_spell = SymSpell(max_dictionary_edit_distance=3, prefix_length=7)\n",
        "dictionary_path = pkg_resources.resource_filename(\n",
        "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
        "if sym_spell.word_count:\n",
        "    pass\n",
        "else:\n",
        "    sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
        "\n",
        "# cohernece\n",
        "from collections import Counter\n",
        "from sklearn.metrics import silhouette_score\n",
        "from gensim.models.coherencemodel import CoherenceModel    \n",
        "import umap\n",
        "from wordcloud import WordCloud\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# lda\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from gensim import corpora\n",
        "import gensim\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "\n",
        "# autoencoder\n",
        "import tensorflow\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF7MdfVP6Z3f"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OwrOUsE6Z3g"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "###################################\n",
        "#### sentence level preprocess ####\n",
        "###################################\n",
        "\n",
        "# lowercase + base filter\n",
        "# some basic normalization\n",
        "def f_base(s):\n",
        "    \"\"\"\n",
        "    :param s: string to be processed\n",
        "    :return: processed string: see comments in the source code for more info\n",
        "    \"\"\"\n",
        "    # normalization 1: xxxThis is a --> xxx. This is a (missing delimiter)\n",
        "    s = re.sub(r'([a-z])([A-Z])', r'\\1\\. \\2', s)  # before lower case\n",
        "    # normalization 2: lower case\n",
        "    s = s.lower()\n",
        "    # normalization 3: \"&gt\", \"&lt\"\n",
        "    s = re.sub(r'&gt|&lt', ' ', s)\n",
        "    # normalization 4: letter repetition (if more than 2)\n",
        "    s = re.sub(r'([a-z])\\1{2,}', r'\\1', s)\n",
        "    # normalization 5: non-word repetition (if more than 1)\n",
        "    s = re.sub(r'([\\W+])\\1{1,}', r'\\1', s)\n",
        "    # normalization 6: string * as delimiter\n",
        "    s = re.sub(r'\\*|\\W\\*|\\*\\W', '. ', s)\n",
        "    # normalization 7: stuff in parenthesis, assumed to be less informal\n",
        "    s = re.sub(r'\\(.*?\\)', '. ', s)\n",
        "    # normalization 8: xxx[?!]. -- > xxx.\n",
        "    s = re.sub(r'\\W+?\\.', '.', s)\n",
        "    # normalization 9: [.?!] --> [.?!] xxx\n",
        "    s = re.sub(r'(\\.|\\?|!)(\\w)', r'\\1 \\2', s)\n",
        "    # normalization 10: ' ing ', noise text\n",
        "    s = re.sub(r' ing ', ' ', s)\n",
        "    # normalization 11: noise text\n",
        "    s = re.sub(r'product received for free[.| ]', ' ', s)\n",
        "    # normalization 12: phrase repetition\n",
        "    s = re.sub(r'(.{2,}?)\\1{1,}', r'\\1', s)\n",
        "\n",
        "    return s.strip()\n",
        "\n",
        "\n",
        "# language detection\n",
        "def f_lan(s):\n",
        "    \"\"\"\n",
        "    :param s: string to be processed\n",
        "    :return: boolean (s is English)\n",
        "    \"\"\"\n",
        "\n",
        "    # some reviews are actually english but biased toward french\n",
        "    return detect_language(s) in {'English', 'French', 'Spanish', 'Chinese'}\n",
        "\n",
        "\n",
        "###############################\n",
        "#### word level preprocess ####\n",
        "###############################\n",
        "\n",
        "# filtering out punctuations and numbers\n",
        "def f_punct(w_list):\n",
        "    \"\"\"\n",
        "    :param w_list: word list to be processed\n",
        "    :return: w_list with punct and number filter out\n",
        "    \"\"\"\n",
        "    return [word for word in w_list if word.isalpha()]\n",
        "\n",
        "\n",
        "# selecting nouns\n",
        "def f_noun(w_list):\n",
        "    \"\"\"\n",
        "    :param w_list: word list to be processed\n",
        "    :return: w_list with only nouns selected\n",
        "    \"\"\"\n",
        "    return [word for (word, pos) in nltk.pos_tag(w_list) if pos[:2] == 'NN']\n",
        "\n",
        "\n",
        "# typo correction\n",
        "def f_typo(w_list):\n",
        "    \"\"\"\n",
        "    :param w_list: word list to be processed\n",
        "    :return: w_list with typo fixed by symspell. words with no match up will be dropped\n",
        "    \"\"\"\n",
        "    w_list_fixed = []\n",
        "    for word in w_list:\n",
        "        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=3)\n",
        "        if suggestions:\n",
        "            w_list_fixed.append(suggestions[0].term)\n",
        "        else:\n",
        "            pass\n",
        "            # do word segmentation, deprecated for inefficiency\n",
        "            # w_seg = sym_spell.word_segmentation(phrase=word)\n",
        "            # w_list_fixed.extend(w_seg.corrected_string.split())\n",
        "    return w_list_fixed\n",
        "\n",
        "\n",
        "# stemming if doing word-wise\n",
        "p_stemmer = PorterStemmer()\n",
        "\n",
        "\n",
        "def f_stem(w_list):\n",
        "    \"\"\"\n",
        "    :param w_list: word list to be processed\n",
        "    :return: w_list with stemming\n",
        "    \"\"\"\n",
        "    return [p_stemmer.stem(word) for word in w_list]\n",
        "\n",
        "\n",
        "# filtering out stop words\n",
        "# create English stop words list\n",
        "\n",
        "stop_words = (list(\n",
        "    set(get_stop_words('en'))\n",
        "    | set(get_stop_words('es'))\n",
        "    | set(get_stop_words('de'))\n",
        "    | set(get_stop_words('it'))\n",
        "    | set(get_stop_words('ca'))\n",
        "    # |set(get_stop_words('cy'))\n",
        "    | set(get_stop_words('pt'))\n",
        "    # |set(get_stop_words('tl'))\n",
        "    | set(get_stop_words('pl'))\n",
        "    # |set(get_stop_words('et'))\n",
        "    | set(get_stop_words('da'))\n",
        "    | set(get_stop_words('ru'))\n",
        "    # |set(get_stop_words('so'))\n",
        "    | set(get_stop_words('sv'))\n",
        "    | set(get_stop_words('sk'))\n",
        "    # |set(get_stop_words('cs'))\n",
        "    | set(get_stop_words('nl'))\n",
        "    # |set(get_stop_words('sl'))\n",
        "    # |set(get_stop_words('no'))\n",
        "    # |set(get_stop_words('zh-cn'))\n",
        "))\n",
        "\n",
        "\n",
        "def f_stopw(w_list):\n",
        "    \"\"\"\n",
        "    filtering out stop words\n",
        "    \"\"\"\n",
        "    return [word for word in w_list if word not in stop_words]\n",
        "\n",
        "\n",
        "def preprocess_sent(rw):\n",
        "    \"\"\"\n",
        "    Get sentence level preprocessed data from raw review texts\n",
        "    :param rw: review to be processed\n",
        "    :return: sentence level pre-processed review\n",
        "    \"\"\"\n",
        "    s = f_base(rw)\n",
        "    if not f_lan(s):\n",
        "        return None\n",
        "    return s\n",
        "\n",
        "\n",
        "def preprocess_word(s):\n",
        "    \"\"\"\n",
        "    Get word level preprocessed data from preprocessed sentences\n",
        "    including: remove punctuation, select noun, fix typo, stem, stop_words\n",
        "    :param s: sentence to be processed\n",
        "    :return: word level pre-processed review\n",
        "    \"\"\"\n",
        "    if not s:\n",
        "        return None\n",
        "    w_list = word_tokenize(s)\n",
        "    w_list = f_punct(w_list)\n",
        "    w_list = f_noun(w_list)\n",
        "    w_list = f_typo(w_list)\n",
        "    w_list = f_stem(w_list)\n",
        "    w_list = f_stopw(w_list)\n",
        "\n",
        "    return w_list\n",
        "\n",
        "\n",
        "def preprocess(docs):\n",
        "    \"\"\"\n",
        "    objective: preprocess the raw text and return clean text and tokenized text\n",
        "    :param docs: input raw text\n",
        "    :return: sentences and clean token\n",
        "    \"\"\"\n",
        "    # calculate total len for document\n",
        "    n_docs = len(docs)\n",
        "    print('[INFO] Preprocessing {} raw texts ...'.format(n_docs))\n",
        "    sentences = []  # sentence level preprocessed\n",
        "    token_lists = []  # word level preprocessed\n",
        "\n",
        "    for idx in range(n_docs):\n",
        "        sentence = preprocess_sent(docs[idx])\n",
        "        token_list = preprocess_word(sentence)\n",
        "        if token_list:\n",
        "            sentences.append(sentence)\n",
        "            token_lists.append(token_list)\n",
        "        print('{} %'.format(str(np.round((idx + 1) / n_docs * 100, 2))), end='\\r')\n",
        "    print(\"[INFO] Total preprocessed sentences: {}\".format(len(sentences)))\n",
        "    print(\"[INFO] Total Tokenized lists: {}\".format(len(token_lists)))\n",
        "    print('[INFO] Preprocessing raw texts. Done!')\n",
        "    return sentences, token_lists\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXPkyb4D6Z3i"
      },
      "source": [
        "## Utility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zoo1jxnI6Z3j"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_topic_words(token_lists, labels, k=None):\n",
        "    \"\"\"\n",
        "    objective : get top words within each topic from clustering results\n",
        "    :param token_lists: all tokens for each documents\n",
        "    :param labels: labels associated with that documents\n",
        "    :param k:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    print(\"[INFO] Getting top words for each topic from clustering result\")\n",
        "    if k is None:\n",
        "        k = len(np.unique(labels))\n",
        "    topics = ['' for _ in range(k)]\n",
        "    for i, c in enumerate(token_lists):\n",
        "        topics[labels[i]] += (' ' + ' '.join(c))\n",
        "    word_counts = list(map(lambda x: Counter(x.split()).items(), topics))\n",
        "    # get sorted word counts\n",
        "    word_counts = list(map(lambda x: sorted(x, key=lambda x: x[1], reverse=True), word_counts))\n",
        "    # get topics\n",
        "    topics = list(map(lambda x: list(map(lambda x: x[0], x[:10])), word_counts))\n",
        "    return topics\n",
        "\n",
        "\n",
        "def get_coherence(model, token_lists, measure='c_v'):\n",
        "    \"\"\"\n",
        "    objective : Get model coherence from gensim.models.coherencemodel\n",
        "    :param model: Topic_Model object\n",
        "    :param token_lists: token lists of docs\n",
        "    :param measure: coherence metrics\n",
        "    :return: coherence score\n",
        "    \"\"\"\n",
        "    if model.method == 'LDA':\n",
        "        print(\"[INFO] Calculating Coherence Score For LDA...\")\n",
        "        cm = CoherenceModel(model=model.ldamodel, texts=token_lists, corpus=model.corpus, dictionary=model.dictionary,\n",
        "                            coherence=measure)\n",
        "        print(\"[INFO] Coherence Score: {}\".format(cm))\n",
        "    else:\n",
        "        print(\"[INFO] Calculating Coherence Score for others (lda_bert, bert, tfidf)\")\n",
        "        topics = get_topic_words(token_lists, model.cluster_model.labels_)\n",
        "        cm = CoherenceModel(topics=topics, texts=token_lists, corpus=model.corpus, dictionary=model.dictionary,\n",
        "                            coherence=measure)\n",
        "        cs = cm.get_coherence()\n",
        "        print(\"[INFO] Coherence Score: {}\".format(cs))\n",
        "    return cs\n",
        "\n",
        "\n",
        "def get_silhouette(model):\n",
        "    \"\"\"\n",
        "    objective: Get silhouette score from model\n",
        "    :param model: Topic_Model object\n",
        "    :return: silhouette score\n",
        "    \"\"\"\n",
        "    if model.method == 'LDA':\n",
        "        print(\"[INFO] silhouette scores not applicable for LDA\")\n",
        "        return\n",
        "    print(\"[INFO] Calculating silhouette scores for clusters..\")\n",
        "    lbs = model.cluster_model.labels_\n",
        "    vec = model.vec[model.method]\n",
        "    sc = silhouette_score(vec, lbs)\n",
        "    print(\"[INFO] Silhouette scores: {}\".format(sc))\n",
        "    return sc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQpFncBe6Z3j"
      },
      "source": [
        "## VISULISATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vft-6Iui6Z3j"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot_proj(embedding, lbs):\n",
        "    \"\"\"\n",
        "    Objective: Plot UMAP embeddings\n",
        "    :param embedding: UMAP (or other) embeddings\n",
        "    :param lbs: labels\n",
        "    \"\"\"\n",
        "    n = len(embedding)\n",
        "    counter = Counter(lbs)\n",
        "    for i in range(len(np.unique(lbs))):\n",
        "        plt.plot(embedding[:, 0][lbs == i], embedding[:, 1][lbs == i], '.', alpha=0.5,\n",
        "                 label='cluster {}: {:.2f}%'.format(i, counter[i] / n * 100))\n",
        "    plt.legend()\n",
        "\n",
        "\n",
        "def visualize(model):\n",
        "    \"\"\"\n",
        "    Visualize the result for the topic model by 2D embedding (UMAP)\n",
        "    :param model: Topic_Model object\n",
        "    \"\"\"\n",
        "    if model.method == 'LDA':\n",
        "        return\n",
        "    reducer = umap.UMAP()\n",
        "    print('[INFO] Calculating UMAP projection ...')\n",
        "    vec_umap = reducer.fit_transform(model.vec[model.method])\n",
        "    print('[INFO] Calculating UMAP projection. Done!')\n",
        "    plot_proj(vec_umap, model.cluster_model.labels_)\n",
        "    dr = 'images/{}/{}'.format(model.method, model.id)\n",
        "    if not os.path.exists(dr):\n",
        "        os.makedirs(dr)\n",
        "    plt.savefig(dr + '/2D_vis')\n",
        "\n",
        "def get_wordcloud(model, token_lists, topic):\n",
        "    \"\"\"\n",
        "    Get word cloud of each topic from fitted model\n",
        "    :param model: Topic_Model object\n",
        "    :param sentences: preprocessed sentences from docs\n",
        "    \"\"\"\n",
        "    if model.method == 'LDA':\n",
        "        return\n",
        "    print('[INFO] Getting wordcloud for topic {} ...'.format(topic))\n",
        "    lbs = model.cluster_model.labels_\n",
        "    tokens = ' '.join([' '.join(_) for _ in np.array(token_lists)[lbs == topic]])\n",
        "\n",
        "    wordcloud = WordCloud(width=800, height=560,\n",
        "                          background_color='white', collocations=False,\n",
        "                          min_font_size=10).generate(tokens)\n",
        "\n",
        "    # plot the WordCloud image\n",
        "    plt.figure(figsize=(8, 5.6), facecolor=None)\n",
        "    plt.imshow(wordcloud)\n",
        "    plt.axis(\"off\")\n",
        "    plt.tight_layout(pad=0)\n",
        "    dr = 'images/{}/{}'.format(model.method, model.id)\n",
        "    if not os.path.exists(dr):\n",
        "        os.makedirs(dr)\n",
        "    plt.savefig(dr + '/Topic' + str(topic) + '_wordcloud')\n",
        "    print('[INFO] Getting wordcloud for topic {}. Done!'.format(topic))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pXEVtrU6Z3k"
      },
      "source": [
        "### Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzP14Ijw6Z3k"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class Autoencoder:\n",
        "    \"\"\"\n",
        "    Autoencoder for learning latent space representation\n",
        "    architecture simplified for only one hidden layer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, latent_dim=32, activation='relu', epochs=200, batch_size=128):\n",
        "        self.latent_dim = latent_dim\n",
        "        self.activation = activation\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.autoencoder = None\n",
        "        self.encoder = None\n",
        "        self.decoder = None\n",
        "        self.his = None\n",
        "\n",
        "    def _compile(self, input_dim):\n",
        "        \"\"\"\n",
        "        compile the computational graph\n",
        "        \"\"\"\n",
        "        input_vec = Input(shape=(input_dim,))\n",
        "        encoded = Dense(self.latent_dim, activation=self.activation)(input_vec)\n",
        "        decoded = Dense(input_dim, activation=self.activation)(encoded)\n",
        "        self.autoencoder = Model(input_vec, decoded)\n",
        "        self.encoder = Model(input_vec, encoded)\n",
        "        encoded_input = Input(shape=(self.latent_dim,))\n",
        "        decoder_layer = self.autoencoder.layers[-1]\n",
        "        self.decoder = Model(encoded_input, self.autoencoder.layers[-1](encoded_input))\n",
        "        self.autoencoder.compile(optimizer='adam', loss=tensorflow.keras.losses.mean_squared_error)\n",
        "\n",
        "    def fit(self, X):\n",
        "        if not self.autoencoder:\n",
        "            self._compile(X.shape[1])\n",
        "        X_train, X_test = train_test_split(X)\n",
        "        self.his = self.autoencoder.fit(X_train, X_train,\n",
        "                                        epochs=self.epochs,\n",
        "                                        batch_size=self.batch_size,\n",
        "                                        shuffle=True,\n",
        "                                        validation_data=(X_test, X_test), verbose=0)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92umquKc6Z3k"
      },
      "source": [
        "### BASE TM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Fr1_cUc6Z3l"
      },
      "outputs": [],
      "source": [
        "\n",
        "# define model object\n",
        "class Topic_Model:\n",
        "    def __init__(self, k=10, method='TFIDF'):\n",
        "        \"\"\"\n",
        "        :param k: number of topics\n",
        "        :param method: method chosen for the topic model\n",
        "        \"\"\"\n",
        "        if method not in {'TFIDF', 'LDA', 'BERT', 'LDA_BERT'}:\n",
        "            raise Exception('Invalid method!')\n",
        "\n",
        "        self.k = k\n",
        "        self.dictionary = None\n",
        "        self.corpus = None\n",
        "        # self.stopwords = None\n",
        "        self.cluster_model = None\n",
        "        self.ldamodel = None\n",
        "        self.vec = {}\n",
        "        self.gamma = 15  # parameter for relative importance of lda\n",
        "        self.method = method\n",
        "        self.AE = None\n",
        "        self.id = method + '_' + datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
        "        print(\"[INFO] Topic Model object created for method: {}\".format(self.method))\n",
        "\n",
        "    def vectorize(self, sentences, token_lists, method=None):\n",
        "        \"\"\"\n",
        "        objective:  Get vector representations from selected methods\n",
        "        :param sentences: input sentences required for clustering\n",
        "        :param token_lists: tokens list required for Topic Modeling\n",
        "        :param method: method for Topic Modeling\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "\n",
        "        # Default method\n",
        "        if method is None:\n",
        "            method = self.method\n",
        "\n",
        "        # turn tokenized documents into a id <-> term dictionary\n",
        "        self.dictionary = corpora.Dictionary(token_lists)\n",
        "        # convert tokenized documents into a document-term matrix\n",
        "        self.corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n",
        "\n",
        "        if method == 'TFIDF':\n",
        "            print('[INFO] Getting vector representations for TF-IDF ...')\n",
        "            tfidf = TfidfVectorizer()\n",
        "            vec = tfidf.fit_transform(sentences)\n",
        "            print('[INFO] Getting vector representations for TF-IDF. Done!')\n",
        "            print(\"[INFO] TFIDF vector shape: {}\".format(vec.shape))\n",
        "            return vec\n",
        "\n",
        "        elif method == 'LDA':\n",
        "            print('[INFO] Getting vector representations for LDA ...')\n",
        "            if not self.ldamodel:\n",
        "                self.ldamodel = gensim.models.ldamodel.LdaModel(self.corpus, num_topics=self.k, id2word=self.dictionary,\n",
        "                                                                passes=20)\n",
        "\n",
        "            def get_vec_lda(model, corpus, k):\n",
        "                \"\"\"\n",
        "                objective: Get the LDA vector representation (probabilistic topic assignments for all documents)\n",
        "                :param model:\n",
        "                :param corpus:\n",
        "                :param k:\n",
        "                :return: vec_lda with dimension: (n_doc * n_topic)\n",
        "                \"\"\"\n",
        "\n",
        "                n_doc = len(corpus)\n",
        "                vec_lda = np.zeros((n_doc, k))\n",
        "                for i in range(n_doc):\n",
        "                    # get the distribution for the i-th document in corpus\n",
        "                    for topic, prob in model.get_document_topics(corpus[i]):\n",
        "                        vec_lda[i, topic] = prob\n",
        "\n",
        "                return vec_lda\n",
        "\n",
        "            vec = get_vec_lda(self.ldamodel, self.corpus, self.k)\n",
        "            print('[INFO] Getting vector representations for LDA. Done!')\n",
        "            print(\"[INFO] LDA Vector shape: {}\".format(vec.shape))\n",
        "            return vec\n",
        "\n",
        "        elif method == 'BERT':\n",
        "\n",
        "            print('[INFO] Getting vector representations for BERT ...')\n",
        "            from sentence_transformers import SentenceTransformer\n",
        "            model = SentenceTransformer('bert-base-nli-max-tokens')\n",
        "            vec = np.array(model.encode(sentences, show_progress_bar=True))\n",
        "            print('[INFO] Getting vector representations for BERT. Done!')\n",
        "            print(\"[INFO] BERT vector shape: {}\".format(vec.shape))\n",
        "            return vec\n",
        "\n",
        "\n",
        "        elif method == 'LDA_BERT':\n",
        "\n",
        "            vec_lda = self.vectorize(sentences, token_lists, method='LDA')\n",
        "            vec_bert = self.vectorize(sentences, token_lists, method='BERT')\n",
        "            print(\"[INFO] Concatenating lda vector and bert vector...\")\n",
        "            vec_ldabert = np.c_[vec_lda * self.gamma, vec_bert]\n",
        "            print(\"[INFO] LDA_BERT vector shape : {}\".format(vec_ldabert.shape))\n",
        "            self.vec['LDA_BERT_FULL'] = vec_ldabert\n",
        "            if not self.AE:\n",
        "                self.AE = Autoencoder()\n",
        "                print('[INFO] Fitting Autoencoder on LDA_BERT vec...')\n",
        "                self.AE.fit(vec_ldabert)\n",
        "                print('[INFO] Fitting Autoencoder Done!')\n",
        "            vec = self.AE.encoder.predict(vec_ldabert)\n",
        "            print(\"[INFO] Autoencoder vector shape: {}\".format(vec.shape))\n",
        "            return vec\n",
        "\n",
        "    def fit(self, sentences, token_lists, method=None, m_clustering=None):\n",
        "        \"\"\"\n",
        "        objective: Fit the topic model for selected method given the preprocessed data\n",
        "        :param sentences: input sentences required for clustering\n",
        "        :param token_lists: tokens list required for Topic Modeling\n",
        "        :param method: method for Topic Modeling\n",
        "        :param m_clustering: no of cluster\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "\n",
        "        # Default method\n",
        "        if method is None:\n",
        "            method = self.method\n",
        "        # Default clustering method\n",
        "        if m_clustering is None:\n",
        "            m_clustering = KMeans\n",
        "\n",
        "        # turn tokenized documents into a id <-> term dictionary\n",
        "        if not self.dictionary:\n",
        "            self.dictionary = corpora.Dictionary(token_lists)\n",
        "            # convert tokenized documents into a document-term matrix\n",
        "            self.corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n",
        "\n",
        "        ####################################################\n",
        "        #### Getting ldamodel or vector representations ####\n",
        "        ####################################################\n",
        "\n",
        "        if method == 'LDA':\n",
        "            if not self.ldamodel:\n",
        "                print('[INFO] Fitting LDA MODEL for Topic Modeling...')\n",
        "                self.ldamodel = gensim.models.ldamodel.LdaModel(self.corpus, num_topics=self.k, id2word=self.dictionary,\n",
        "                                                                passes=20)\n",
        "                print('[INFO] Fitting LDA Done!')\n",
        "        else:\n",
        "            print('[INFO] Clustering embeddings ...')\n",
        "            self.cluster_model = m_clustering(self.k)\n",
        "            self.vec[method] = self.vectorize(sentences, token_lists, method)\n",
        "            print(\"[INFO] Fitting Kmeans clustering model..\")\n",
        "            self.cluster_model.fit(self.vec[method])\n",
        "            print(\"[INFO] Fitting Kmeans clustering model Done!\")\n",
        "            print('[INFO] Clustering embeddings. Done!')\n",
        "\n",
        "    def predict(self, sentences, token_lists, out_of_sample=None):\n",
        "        \"\"\"\n",
        "        objective:  Predict topics for new_documents\n",
        "        :param sentences: input sentences required for clustering\n",
        "        :param token_lists: tokens list required for Topic Modeling\n",
        "        :param out_of_sample: True means prediction on new document\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        # Default as False\n",
        "        out_of_sample = out_of_sample is not None\n",
        "        if out_of_sample:\n",
        "            print(\"[INFO] Predicting on New documents..\")\n",
        "        else:\n",
        "            print(\"[INFO] Predicting on same train data\")\n",
        "\n",
        "        if out_of_sample:\n",
        "            corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n",
        "            if self.method != 'LDA':\n",
        "                vec = self.vectorize(sentences, token_lists)\n",
        "                print(vec)\n",
        "        else:\n",
        "            corpus = self.corpus\n",
        "            vec = self.vec.get(self.method, None)\n",
        "\n",
        "        if self.method == \"LDA\":\n",
        "            lbs = np.array(list(map(lambda x: sorted(self.ldamodel.get_document_topics(x),\n",
        "                                                     key=lambda x: x[1], reverse=True)[0][0],\n",
        "                                    corpus)))\n",
        "        else:\n",
        "            lbs = self.cluster_model.predict(vec)\n",
        "        return lbs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Data"
      ],
      "metadata": {
        "id": "C4SBJJsN8FPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kd7W42DC8fuR",
        "outputId": "c74046be-800b-450c-8581-a42bf85fb868"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set project directory\n",
        "os.chdir(\"/content/drive/MyDrive/Sentiment_Project\")\n",
        "\n",
        "# read data\n",
        "df1 = pd.read_excel(\"DATA/ukraine_tweets_sample_copy1.xlsx\")\n",
        "df2 = pd.read_excel(\"DATA/ukraine_tweets_sample_copy2.xlsx\")\n",
        "df3 = pd.read_excel(\"DATA/ukraine_tweets_sample_copy3.xlsx\")\n",
        "\n",
        "# concat all data files and remove irrelevent label\n",
        "df = pd.concat([df1, df2, df3], axis=0)\n",
        "df = df[df[\"Stance Label\"]!=\"Irrelevant\"].reset_index(drop=True)\n",
        "df.to_csv(\"DATA/ALL_Combine_Data.csv\", index=False)\n",
        "print(df.shape)\n",
        "df.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "HriWQCdP8Eoh",
        "outputId": "94d2ad7d-e4a0-4176-cda0-6370941a27c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4177, 4)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                id_str          created_at  \\\n",
              "0  1497026099494023173 2022-02-25 01:50:10   \n",
              "1  1497258644987002880 2022-02-25 17:14:13   \n",
              "2  1497872993732628484 2022-02-27 09:55:25   \n",
              "3  1499163764951818241 2022-03-02 23:24:29   \n",
              "4  1499467511087222789 2022-03-03 19:31:28   \n",
              "5  1508139059427819522 2022-03-27 17:49:06   \n",
              "6  1508675556358774786 2022-03-29 05:20:57   \n",
              "7  1512504441324322834 2022-04-08 18:55:34   \n",
              "8  1515578470755192837 2022-04-17 06:30:40   \n",
              "9  1516846493637951489 2022-04-20 18:29:20   \n",
              "\n",
              "                                           full_text Stance Label  \n",
              "0  Russian Rosgvardia troops, likely spetsnaz, cr...      Neutral  \n",
              "1  @WrestlingWombat @HStefansonMB What is happeni...   ProUkraine  \n",
              "2  Today on the campaign trail I had some tough c...   ProUkraine  \n",
              "3  ICYMI: n light of Russia sanctions, Fed chair ...      Neutral  \n",
              "4  By now over a thousand refugees from #Ukraine ...   ProUkraine  \n",
              "5  Whoever is coming up with reactive strategy by...    ProRussia  \n",
              "6  He also writes about the possibility of Ukrain...   ProUkraine  \n",
              "7  At Horizon, we are deeply concerned and sadden...   ProUkraine  \n",
              "8  #UkraineUnderAttack Join us in this Democracy ...   ProUkraine  \n",
              "9  @RALee85 Russian tanks in cities where they’re...   ProUkraine  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a6d6bf3d-24d1-4a10-82be-48a69cd4af31\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id_str</th>\n",
              "      <th>created_at</th>\n",
              "      <th>full_text</th>\n",
              "      <th>Stance Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1497026099494023173</td>\n",
              "      <td>2022-02-25 01:50:10</td>\n",
              "      <td>Russian Rosgvardia troops, likely spetsnaz, cr...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1497258644987002880</td>\n",
              "      <td>2022-02-25 17:14:13</td>\n",
              "      <td>@WrestlingWombat @HStefansonMB What is happeni...</td>\n",
              "      <td>ProUkraine</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1497872993732628484</td>\n",
              "      <td>2022-02-27 09:55:25</td>\n",
              "      <td>Today on the campaign trail I had some tough c...</td>\n",
              "      <td>ProUkraine</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1499163764951818241</td>\n",
              "      <td>2022-03-02 23:24:29</td>\n",
              "      <td>ICYMI: n light of Russia sanctions, Fed chair ...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1499467511087222789</td>\n",
              "      <td>2022-03-03 19:31:28</td>\n",
              "      <td>By now over a thousand refugees from #Ukraine ...</td>\n",
              "      <td>ProUkraine</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1508139059427819522</td>\n",
              "      <td>2022-03-27 17:49:06</td>\n",
              "      <td>Whoever is coming up with reactive strategy by...</td>\n",
              "      <td>ProRussia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1508675556358774786</td>\n",
              "      <td>2022-03-29 05:20:57</td>\n",
              "      <td>He also writes about the possibility of Ukrain...</td>\n",
              "      <td>ProUkraine</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1512504441324322834</td>\n",
              "      <td>2022-04-08 18:55:34</td>\n",
              "      <td>At Horizon, we are deeply concerned and sadden...</td>\n",
              "      <td>ProUkraine</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1515578470755192837</td>\n",
              "      <td>2022-04-17 06:30:40</td>\n",
              "      <td>#UkraineUnderAttack Join us in this Democracy ...</td>\n",
              "      <td>ProUkraine</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1516846493637951489</td>\n",
              "      <td>2022-04-20 18:29:20</td>\n",
              "      <td>@RALee85 Russian tanks in cities where they’re...</td>\n",
              "      <td>ProUkraine</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a6d6bf3d-24d1-4a10-82be-48a69cd4af31')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a6d6bf3d-24d1-4a10-82be-48a69cd4af31 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a6d6bf3d-24d1-4a10-82be-48a69cd4af31');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqtLIs0T6Z3l"
      },
      "source": [
        "## Coherence Range"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qN2RAuVM6Z3l"
      },
      "outputs": [],
      "source": [
        "# coherence range tune toppic model \n",
        "class CoherenceOverRange:\n",
        "    def __init__(self, input_data_path='DATA/ALL_Combine_Data.csv', method='LDA_BERT',\n",
        "                 ntopic_min=5, ntopic_max=26, step=1, save=True, output_data_path='DATA/output_lda_bert.csv'):\n",
        "        self.input_data_path = input_data_path\n",
        "        self.method = method\n",
        "        self.ntopic_min = ntopic_min\n",
        "        self.ntopic_max = ntopic_max\n",
        "        self.step = step\n",
        "        self.best_tm = None\n",
        "        self.best_ntopic = None\n",
        "        self.best_coherence = None\n",
        "        self.save = save\n",
        "        self.output_data_path = output_data_path\n",
        "\n",
        "    def tune_model(self):\n",
        "        documents = pd.read_csv(self.input_data_path)\n",
        "        documents = documents.iloc[:, 0:2]\n",
        "        samp_size = len(documents)\n",
        "        documents.columns = ['index', 'abstract']\n",
        "        documents.dropna(inplace=True)\n",
        "        documents.abstract = documents.abstract.astype(str)\n",
        "        rws = documents.abstract\n",
        "        sentences, token_lists, idx_in = preprocess(rws, samp_size=samp_size)\n",
        "\n",
        "        Coherence = []\n",
        "        models = []\n",
        "        ntopics = range(self.ntopic_min, self.ntopic_max, self.step)\n",
        "        for ntopic in ntopics:\n",
        "            # Define the topic model object\n",
        "            tm = Topic_Model(k=ntopic, method=self.method)\n",
        "            # Fit the topic model by chosen method\n",
        "            tm.fit(sentences, token_lists)\n",
        "            # Evaluate using metrics\n",
        "            c = get_coherence(tm, token_lists, 'c_v')\n",
        "            print('Number of Topics:', ntopic)\n",
        "            print('Coherence:', c)\n",
        "            Coherence.append(c)\n",
        "            models.append(tm)\n",
        "\n",
        "        # initialize data of lists.\n",
        "        coherence_scores = {'n_topics': ntopics, 'coherence_' + self.method: Coherence}\n",
        "\n",
        "        output = []\n",
        "        scores = []\n",
        "        for i in zip(models, Coherence, ntopics):\n",
        "            output.append(i)\n",
        "            scores.append(i[1])\n",
        "\n",
        "        max_score = sorted(scores)[-1]\n",
        "        for model, score, num_topics in output:\n",
        "            if score == max_score:\n",
        "                print(f'Selected LDA_BERT configuration: {num_topics} with Coherence score: {score}')\n",
        "                self.best_tm = model\n",
        "                self.best_ntopic = num_topics\n",
        "                self.best_coherence = score\n",
        "\n",
        "        # Store the best model version\n",
        "        if self.save:\n",
        "            pickle_file = open('../models/topic_model.pkl', 'wb')\n",
        "            pickle.dump(self.best_tm, pickle_file)\n",
        "            pickle_file.close()\n",
        "\n",
        "        # Evaluate using metrics\n",
        "        if self.method == \"LDA\":\n",
        "            topics = []\n",
        "            for topic in self.best_tm.ldamodel.print_topics(num_topics=self.best_ntopic):\n",
        "                topic_list = [re.search(r\"[a-zA-z]+\", w).group(0) for w in topic[1].split('+')]\n",
        "                topics.append(topic_list)\n",
        "        else:\n",
        "            topics = get_topic_words(token_lists, tm.cluster_model.labels_)\n",
        "        labels = tm.predict(sentences, token_lists)\n",
        "\n",
        "        predictions = documents.copy()\n",
        "        predictions['topic_no'] = np.NaN\n",
        "        # Mapping the queries with their labels\n",
        "        count = 0\n",
        "        for idx in idx_in:\n",
        "            predictions['topic_no'][idx] = labels[count]\n",
        "            count = count + 1\n",
        "\n",
        "        mapped_predictions = predictions.groupby('topic_no')['abstract'].apply(list)\n",
        "        mapped_predictions = mapped_predictions.reset_index()\n",
        "        mapped_predictions.sort_values(by=['topic_no'], inplace=True)\n",
        "        mapped_predictions['keywords_list'] = topics\n",
        "        mapped_predictions.abstract = mapped_predictions.abstract.apply(lambda x: x[:500])\n",
        "        if self.save:\n",
        "            mapped_predictions.to_csv(self.output_data_path)\n",
        "        return mapped_predictions, coherence_scores\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     input_p = 'data/bbc-text.csv'\n",
        "#     output_p = 'data/bbc-text-output.csv'\n",
        "#     compute_co = CoherenceOverRange(input_data_path=input_p, output_data_path=output_p)\n",
        "#     output_data, coherence_scores = compute_co.tune_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuBRI28y6Z3l"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No0X63N_6Z3l"
      },
      "source": [
        "### Train Topic Modeling "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5q1Yq8Jg6Z3m"
      },
      "source": [
        "### main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKLHFanA6Z3m"
      },
      "outputs": [],
      "source": [
        "\n",
        "class LDABertModel:\n",
        "\n",
        "    def __init__(self, input_data_path='\"DATA/ALL_Combine_Data.csv\"', method='LDA_BERT',\n",
        "                 ntopic=10, save=True, output_data_path='DATA/output_bert.csv\"'):\n",
        "        self.input_data_path = input_data_path\n",
        "        self.method = method\n",
        "        self.ntopic = ntopic\n",
        "        self.save = save\n",
        "        self.output_data_path = output_data_path\n",
        "\n",
        "    def train(self):\n",
        "        # Reading the dataset\n",
        "        documents = pd.read_csv(self.input_data_path)\n",
        "\n",
        "        # keeping only the first two columns and 100 documents [index, text]\n",
        "        documents = documents.iloc[:100, 0:2]\n",
        "\n",
        "        # drop missing\n",
        "        documents.dropna(inplace=True)\n",
        "        print(\"[INFO] all columns: \", documents.columns)\n",
        "\n",
        "        # rename columns\n",
        "        documents.columns = ['index', 'text']\n",
        "\n",
        "        # convert datatype to str\n",
        "        documents.abstract = documents.text.astype(str)\n",
        "\n",
        "        # select all raw text documents\n",
        "        rws = documents.abstract\n",
        "\n",
        "        # preprocess the text\n",
        "        sentences, token_lists = preprocess(rws)\n",
        "\n",
        "        # check for empty tokens\n",
        "        empty = 0\n",
        "        for t in token_lists:\n",
        "            if len(t) == 0:\n",
        "                empty += 1\n",
        "        print(\"[INFO] Count For empty tokens: \", empty)\n",
        "\n",
        "        # Define the topic model object\n",
        "        tm = Topic_Model(k=self.ntopic, method=self.method)\n",
        "\n",
        "        # Fit the topic model by chosen method\n",
        "        tm.fit(sentences, token_lists)\n",
        "        # Evaluate using metrics\n",
        "        if self.method == \"LDA\":\n",
        "            topics = []\n",
        "            for topic in tm.ldamodel.print_topics(num_topics=self.ntopic):\n",
        "                topic_list = [re.search(r\"[a-zA-z]+\", w).group(0) for w in topic[1].split('+')]\n",
        "                topics.append(topic_list)\n",
        "        else:\n",
        "            topics = get_topic_words(token_lists, tm.cluster_model.labels_)\n",
        "        labels = tm.predict(sentences, token_lists)\n",
        "\n",
        "        c = get_coherence(tm, token_lists, 'c_v')\n",
        "        s = get_silhouette(tm)\n",
        "        print('[INFO] Number of Topics:', self.ntopic)\n",
        "\n",
        "        # visualize and save img\n",
        "        # visualize(tm)\n",
        "        # for i in range(tm.k):\n",
        "        #     get_wordcloud(tm, token_lists, i)\n",
        "\n",
        "        predictions = documents.copy()\n",
        "        predictions['topic_no'] = np.NaN\n",
        "        # Mapping the queries with their labels\n",
        "\n",
        "        predictions['topic_no'] = labels\n",
        "        print(\"final output\")\n",
        "        print(predictions['topic_no'].value_counts())\n",
        "\n",
        "        \n",
        "        if self.save:\n",
        "            predictions.to_csv(self.output_data_path)\n",
        "        return predictions\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     input_p = \"../data/bbc-text.csv\"\n",
        "#     output_p = \"../output/bbc-text_reviews.csv\"\n",
        "#     topic_model = LDABertModel(input_data_path=input_p, ntopic=5, output_data_path=output_p)\n",
        "#     output_data = topic_model.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zU6TtRc06Z3m"
      },
      "outputs": [],
      "source": [
        "input_data_path = \"DATA/ALL_Combine_Data.csv\"\n",
        "method ='LDA_BERT'\n",
        "ntopic = 5\n",
        "save=True\n",
        "output_data_path = \"output/ALL_Combine_Data_Output.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yaCmM3C6Z3m",
        "outputId": "a3621ecb-7844-48ae-b9b7-7ecbc9e728de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] all columns:  Index(['id_str', 'full_text'], dtype='object')\n",
            "[INFO] Preprocessing 4177 raw texts ...\n",
            "[INFO] Total preprocessed sentences: 4158\n",
            "[INFO] Total Tokenized lists: 4158\n",
            "[INFO] Preprocessing raw texts. Done!\n",
            "[INFO] Count For empty tokens:  0\n"
          ]
        }
      ],
      "source": [
        "# Reading the dataset\n",
        "documents = pd.read_csv(input_data_path)\n",
        "\n",
        "# keeping only the first two columns and 100 documents [index, text]\n",
        "documents = documents[[\"id_str\",\"full_text\"]]\n",
        "\n",
        "# drop missing\n",
        "documents.dropna(inplace=True)\n",
        "print(\"[INFO] all columns: \", documents.columns)\n",
        "\n",
        "# rename columns\n",
        "documents.columns = ['index', 'text']\n",
        "\n",
        "# convert datatype to str\n",
        "documents.abstract = documents.text.astype(str)\n",
        "\n",
        "# select all raw text documents\n",
        "rws = documents.abstract\n",
        "\n",
        "# preprocess the text\n",
        "sentences, token_lists = preprocess(rws)\n",
        "\n",
        "# check for empty tokens\n",
        "empty = 0\n",
        "for t in token_lists:\n",
        "    if len(t) == 0:\n",
        "        empty += 1\n",
        "print(\"[INFO] Count For empty tokens: \", empty)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344,
          "referenced_widgets": [
            "99a5c47d626040f9a64c7f6ce0e095cc",
            "ff1dcb5d20f34e4c8babdd8b388b028b",
            "a6bb7e4223ec4e3b8cf35d0060f5bbff",
            "16951be2deed4462bdf9d32a186b35c7",
            "923037b0faa540f3ba0ed3b1d9e30ff5",
            "7f3115acb8a943da81030e320200590f",
            "dc3c8243c3fd4e94a2aba8608b963a57",
            "d51c244b8b704b70bee6cbdbdbca348b",
            "8f70f0b98b02464eaa7929261946864e",
            "d4664816f3484bd888fc1644e0217d4a",
            "96e2a1757c504be994ba128f7d0422a8"
          ]
        },
        "id": "CH8_RpgG6Z3m",
        "outputId": "c211b02c-e6cc-4993-be9f-3ce1fc8eecaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Topic Model object created for method: LDA_BERT\n",
            "[INFO] Clustering embeddings ...\n",
            "[INFO] Getting vector representations for LDA ...\n",
            "[INFO] Getting vector representations for LDA. Done!\n",
            "[INFO] LDA Vector shape: (4158, 5)\n",
            "[INFO] Getting vector representations for BERT ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/130 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "99a5c47d626040f9a64c7f6ce0e095cc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Getting vector representations for BERT. Done!\n",
            "[INFO] BERT vector shape: (4158, 768)\n",
            "[INFO] Concatenating lda vector and bert vector...\n",
            "[INFO] LDA_BERT vector shape : (4158, 773)\n",
            "[INFO] Fitting Autoencoder on LDA_BERT vec...\n",
            "[INFO] Fitting Autoencoder Done!\n",
            "130/130 [==============================] - 0s 2ms/step\n",
            "[INFO] Autoencoder vector shape: (4158, 32)\n",
            "[INFO] Fitting Kmeans clustering model..\n",
            "[INFO] Fitting Kmeans clustering model Done!\n",
            "[INFO] Clustering embeddings. Done!\n"
          ]
        }
      ],
      "source": [
        "# Define the topic model object\n",
        "tm = Topic_Model(k=ntopic, method=method)\n",
        "\n",
        "# Fit the topic model by chosen method\n",
        "tm.fit(sentences, token_lists)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GDRBKYk6Z3m",
        "outputId": "8f2b6edc-9e56-4875-f81c-f4f2c34eca76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Getting top words for each topic from clustering result\n"
          ]
        }
      ],
      "source": [
        "# Evaluate using metrics\n",
        "if tm.method == \"LDA\":\n",
        "    topics = []\n",
        "    for topic in tm.ldamodel.print_topics(num_topics=ntopic):\n",
        "        topic_list = [re.search(r\"[a-zA-z]+\", w).group(0) for w in topic[1].split('+')]\n",
        "        topics.append(topic_list)\n",
        "else:\n",
        "    topics = get_topic_words(token_lists, tm.cluster_model.labels_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zMC2CFI6Z3m",
        "outputId": "1cddf72a-e303-44ca-a4a7-d57ac1048c21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 - tip propaganda ukrain russian russia peopl nat year flag news\n",
            "1 - tip putin russia peopl countri day time propaganda world leader\n",
            "2 - tip putin russia sanction weapon invas support countri trump ukrain\n",
            "3 - tip russia putin ukrain ukrainian russian news countri propaganda peopl\n",
            "4 - ukrain russia peopl tip world kerenski putin money countri news\n"
          ]
        }
      ],
      "source": [
        "topic_keywords = dict()\n",
        "for idx , key in enumerate(topics):\n",
        "    topic_keywords[idx] = \" \".join(key) \n",
        "    print(\"{} - {}\".format(idx, \" \".join(key)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0jYjilI6Z3n",
        "outputId": "ecb7c288-79c9-4a7f-e6af-19590c04be35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Predicting on same train data\n",
            "[INFO] Calculating Coherence Score for others (lda_bert, bert, tfidf)\n",
            "[INFO] Getting top words for each topic from clustering result\n",
            "[INFO] Coherence Score: 0.29998767710066804\n",
            "[INFO] Calculating silhouette scores for clusters..\n",
            "[INFO] Silhouette scores: 0.30351853370666504\n",
            "[INFO] Number of Topics: 5\n"
          ]
        }
      ],
      "source": [
        "labels = tm.predict(sentences, token_lists)\n",
        "\n",
        "c = get_coherence(tm, token_lists, 'c_v')\n",
        "s = get_silhouette(tm)\n",
        "print('[INFO] Number of Topics:', ntopic)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jBbM9E8zCpZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "Vxni8EVF6Z3n",
        "outputId": "341dd6eb-b0d9-49b7-9cd5-100cc0978319"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Calculating UMAP projection ...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-3e2d1c3596f4>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# visualize and save img\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvisualize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mget_wordcloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_lists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-8c6c5b53c00c>\u001b[0m in \u001b[0;36mvisualize\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mreducer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mumap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUMAP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[INFO] Calculating UMAP projection ...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mvec_umap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreducer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[INFO] Calculating UMAP projection. Done!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mplot_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec_umap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/umap/umap_.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1596\u001b[0m             \u001b[0mEmbedding\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlow\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdimensional\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m         \"\"\"\n\u001b[0;32m-> 1598\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1599\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/umap/umap_.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1450\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_search_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_knn_dists\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m             self._search_graph = self._search_graph.maximum(\n\u001b[0;32m-> 1452\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_search_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1453\u001b[0m             ).tocsr()\n\u001b[1;32m   1454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/scipy/sparse/_lil.py\u001b[0m in \u001b[0;36mtranspose\u001b[0;34m(self, axes, copy)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocsr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0mtranspose\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/scipy/sparse/_lil.py\u001b[0m in \u001b[0;36mtocsr\u001b[0;34m(self, copy)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mindptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0mindptr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0m_csparsetools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlil_get_lengths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindptr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0mnnz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindptr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mscipy/sparse/_csparsetools.pyx\u001b[0m in \u001b[0;36m_csparsetools.lil_get_lengths\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Buffer has wrong number of dimensions (expected 1, got 2)"
          ]
        }
      ],
      "source": [
        "# visualize and save img\n",
        "visualize(tm)\n",
        "for i in range(tm.k):\n",
        "    get_wordcloud(tm, token_lists, i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3TTM2bM6Z3n"
      },
      "outputs": [],
      "source": [
        "predictions = documents.copy()\n",
        "predictions['topic_no'] = np.NaN\n",
        "\n",
        "# Mapping the queries with their labels\n",
        "predictions['topic_no'] = labels\n",
        "predictions.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLi5yg4A6Z3n"
      },
      "outputs": [],
      "source": [
        "topic_keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3MYIrCb86Z3n"
      },
      "outputs": [],
      "source": [
        "predictions[\"keywords\"] = predictions[\"topic_no\"].map(topic_keywords)\n",
        "predictions.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnyqaHYr6Z3o"
      },
      "outputs": [],
      "source": [
        "## topic volume \n",
        "\n",
        "predictions[\"topic_no\"].value_counts().plot(kind=\"pie\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVv66NFV6Z3o"
      },
      "outputs": [],
      "source": [
        "if save:\n",
        "    predictions.to_csv(\"output/full_output.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prLqjBO86Z3o"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "99a5c47d626040f9a64c7f6ce0e095cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff1dcb5d20f34e4c8babdd8b388b028b",
              "IPY_MODEL_a6bb7e4223ec4e3b8cf35d0060f5bbff",
              "IPY_MODEL_16951be2deed4462bdf9d32a186b35c7"
            ],
            "layout": "IPY_MODEL_923037b0faa540f3ba0ed3b1d9e30ff5"
          }
        },
        "ff1dcb5d20f34e4c8babdd8b388b028b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f3115acb8a943da81030e320200590f",
            "placeholder": "​",
            "style": "IPY_MODEL_dc3c8243c3fd4e94a2aba8608b963a57",
            "value": "Batches: 100%"
          }
        },
        "a6bb7e4223ec4e3b8cf35d0060f5bbff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d51c244b8b704b70bee6cbdbdbca348b",
            "max": 130,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8f70f0b98b02464eaa7929261946864e",
            "value": 130
          }
        },
        "16951be2deed4462bdf9d32a186b35c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4664816f3484bd888fc1644e0217d4a",
            "placeholder": "​",
            "style": "IPY_MODEL_96e2a1757c504be994ba128f7d0422a8",
            "value": " 130/130 [14:47&lt;00:00,  2.29s/it]"
          }
        },
        "923037b0faa540f3ba0ed3b1d9e30ff5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f3115acb8a943da81030e320200590f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc3c8243c3fd4e94a2aba8608b963a57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d51c244b8b704b70bee6cbdbdbca348b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f70f0b98b02464eaa7929261946864e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d4664816f3484bd888fc1644e0217d4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96e2a1757c504be994ba128f7d0422a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}